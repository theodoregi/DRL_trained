{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Copyright 2022-2023 Jean-Luc CHARLES\n",
    "# Created: 2022-07-29\n",
    "# version: 1.2 - 3 Dec 2023\n",
    "# License: GNU GPL-3.0-or-later\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Deep Reinforcement Learning_ (DRL)\n",
    "# Train a PPO neural network control the robot position.\n",
    "\n",
    "In this notebook you will learn how to use DRL to train a ___PPO___  neural network to control the robot position.\n",
    "\n",
    "# Outline <a name=\"top\"></a>\n",
    "- [ 1 $-$ The RoboticArm_2DOF class](#1)\n",
    "    - [ 1.1 $-$ Instancite the robot](#1.1)\n",
    "    - [ 1.2 $-$ Run a simple test](#1.2)\n",
    "- [ 2 $-$ Train the PPO neural network](#2)\n",
    "    - [ 2.1 $-$ A first _fake_ training to see](#2.1)\n",
    "    - [ 2.2 $-$ Run the full training](#2.2)\n",
    "    - [ 2.3 $-$ Find the best training epoch](#2.3)\n",
    "    - [ 2.4 $-$ Evaluate the trained agent performance](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended links:\n",
    "- Pybullet online documentation: [PyBullet Quickstart Guide](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.2ye70wns7io3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, shutil, yaml, pathlib, shutil\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "from numpy.linalg import norm       # to get the norm of a vector\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import is_close_to, display_joint_properties, test_training, sample_line, sample_traj4pts\n",
    "from utils.tools import welcome, plot_test, moving_average, get_files_by_date\n",
    "\n",
    "from utils.RoboticArm_2DOF import RoboticArm_2DOF_PyBullet\n",
    "\n",
    "# the PyBullect connection:\n",
    "pc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. $-$ The RoboticArm_2DOF_PyBullet class <a name=\"1\"></a>\n",
    "\n",
    "The RoboticArm_2DOF class inherits from the base class `env` of the framework _Gymnasium_.<br>\n",
    "It is defined in the file `utils/RoboticArm_2DOF.py` with main tasks : \n",
    "- Create an instance of the Env class\n",
    "- Create a PyBullet session\n",
    "- Instanciate the 2 DOF robot arm in the simulator session using its URDF description file.\n",
    "- Make the simulated robot move under the actions given by the PPO agent.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 $-$ Instanciate the robot <a name=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT  = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir(): \n",
    "    env.close()\n",
    "    del env\n",
    "\n",
    "env = RoboticArm_2DOF_PyBullet(robot_urdf=ROBOT,      # mandatory\n",
    "                               target_urdf=TARGET,    # mandatory\n",
    "                               dt=1/240,              # mandatory, time step of the simulation [s] (~4ms) \n",
    "                               headless=False,        # to get the PyBullet graphical interface \n",
    "                               verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Run a simple kinematic test<a name=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, the two DOFs of the robot (angles $q_1$ and $q_2$) are controlled by value to make the robot move around its start position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = env.testAngleControl(0.075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data:\n",
    "\n",
    "We can verify that the actual values of $q_1$ and $q_2$ are close to the tagrte values (dashed black lines).<br>\n",
    "We also draw the velocities $\\dot{q}_1$ and $\\dot{q}_2$, as well as the trajectory of the end effector of the robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "plot_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the reset() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the environment\n",
    "\n",
    "When you have done with the `env` object, before creating another `RoboticArm_2DOF_PyBullet` it is important to close the current `env` properly in order to close the Pybullet session ans all what is connected with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Train the PPO neural network <a name=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Strategy\n",
    "\n",
    "The training of the robot involves a loop on the target position:\n",
    "- A random target position is choosen in the space reachable by the robot.\n",
    "- During the training, the reward function encourages the robot to move its end effector as close as possible to the target position.\n",
    "- When done (the end effector is close enough to the target), a new target position is randomly choosen, and so on...\n",
    "\n",
    "The whole training process is driven by many __hyperparameters__ including:\n",
    "- `tot_steps`: the total number of random positions learned before we decide that the network is trained.\n",
    "- `EPSILON`: the threshold distance between the end effector and the target mass below which the effector is considered close enough to the target.\n",
    "- `n_epoch`: the (classical) number of training successively run with the same data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A first _fake_ training to see <a name=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run now a first _fake_ training to see if all is OK : the purpose of this section is just to show you the steps of a DRL training scenario. <br>\n",
    "At this stage, you will use the __reward function__ `reward_0` already defined in the file `reward.py` : it returns always 0, so the PPO agent wil not learn anything....<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Instanciate the robot <a name=\"2.1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define important parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 1/240          # the simulation time step [s]\n",
    "EPSILON = 1e-3           # the distance threshold between the end effector and the target\n",
    "SEED    = 1234567        # the seed for the random generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `RoboticArm_2DOF_PyBullet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBOT  = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = ROBOT, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  init_target_pos = (0.5, 0, 0.5),\n",
    "                                  reward = 'reward_0',\n",
    "                                  seed = SEED,\n",
    "                                  epsilon = EPSILON,\n",
    "                                  headless = False,  # turn graphical rendering on for this 'fake' round\n",
    "                                  max_episode_steps = 256,\n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `env.reset` to verify the randomisation of the target and end effector (x,z) positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Instanciate the PPO network <a name=\"2.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the PPO training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy     = 'MlpPolicy'\n",
    "tot_steps  = 20000       # only 10000 steps for this 'fake' round\n",
    "save_freq  = 5000        # save the networks weights every 'save_freq' steps\n",
    "nb_steps   = 2048        # The number of steps to run per update (the size of the rollout buffer)\n",
    "nb_epochs  = 10          # number of training iterations with the same dataset\n",
    "batch_size = 256         # size of the batch to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define automatically a uniq name for the training directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_time = time.localtime()\n",
    "experiment_id = \"_\".join(['2DOF_RobotArm_PyBullet', 'PPO', time.strftime(\"%y-%m-%d_%H-%M-%S\", experiment_time)])\n",
    "\n",
    "training_dir = pathlib.Path('models')/experiment_id\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training in directory <{training_dir}>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the PPO neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "agent = PPO(policy, \n",
    "            env, \n",
    "            n_epochs = nb_epochs,\n",
    "            n_steps = nb_steps,\n",
    "            batch_size = batch_size,\n",
    "            use_sde = False,\n",
    "            seed = SEED,\n",
    "            tensorboard_log = training_dir,\n",
    "            verbose=1)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq = save_freq, \n",
    "                                         save_path = training_dir/'ZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display some details on the actor & critic networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network<br>\n",
    "(you can find some explanations of the training display here : https://stable-baselines3.readthedocs.io/en/master/common/logger.html#rollout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train agent\n",
    "t0 = time.time()\n",
    "\n",
    "agent.learn(total_timesteps = tot_steps, callback = checkpoint_callback)\n",
    "    \n",
    "t = int(time.time()-t0)\n",
    "h = int(t/3600)\n",
    "m = int((t - h*3600)/60)\n",
    "print(f\"Training elapsed time: {h:02d}h {m:02d}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB;font-weight: bold; font-size:large;\">$\\blacktriangleright$ Check target positions:</span>\n",
    "<span style=\"color: #0000BB\">\n",
    "\n",
    "Using the coordinates `x`and `z` of the attribute `env.target_pos` plot the successive positions of the target during training:\n",
    "</span></div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB;font-weight: bold; font-size:large;\">$\\blacktriangleright$  Check robot end effector positions:</span>\n",
    "<span style=\"color: #0000BB\">\n",
    "\n",
    "Using the attribut `env.ee_pos` plot the successive positions of the robot end effector during training:\n",
    "</span></div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we close the environment to restart a new one for the full training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 $-$ Run the full training <a name=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 1/240          # the simulation time step\n",
    "EPSILON = 1e-3           # the distance threshold betwwen the end effector and the target\n",
    "SEED    = 1234567        # the seed for the random generators\n",
    "MAX_EPISODE_STEPS = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `RoboticArm_2DOF_PyBullet` with the right reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = URDF, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  init_target_pos = (0.5, 0, 0.5),\n",
    "                                  reward = 'reward_1',\n",
    "                                  seed = SEED,\n",
    "                                  epsilon = EPSILON,\n",
    "                                  headless = True,  # no more graphical rendering for this round\n",
    "                                  max_episode_steps = MAX_EPISODE_STEPS,\n",
    "                                  verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = 'MlpPolicy'\n",
    "tot_steps  = 5000000     # will take a few hours... (~ 4h on a core_I7 laptop)\n",
    "save_freq  = 100000      # save the networks weights every 'save_freq' steps\n",
    "nb_steps   = 2048        # The number of steps to run per update (the size of the rollout buffer)\n",
    "nb_epochs  = 10          # number of training iterations with the same dataset\n",
    "batch_size = 512         # size of the batch to train the network\n",
    "headless   = True        # no graphical renering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define automatically a uniq name for the training directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_time = time.localtime()\n",
    "experiment_id = \"_\".join(['2DOF_RobotArm_PyBullet', 'PPO', time.strftime(\"%y-%m-%d_%H-%M-%S\", experiment_time)])\n",
    "\n",
    "training_dir = pathlib.Path('models') / experiment_id\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training in directory <{training_dir}>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "agent = PPO(policy, \n",
    "            env, \n",
    "            n_epochs = nb_epochs,\n",
    "            n_steps = nb_steps,\n",
    "            batch_size = batch_size,\n",
    "            use_sde = False,\n",
    "            seed = SEED,\n",
    "            tensorboard_log = training_dir,\n",
    "            verbose = 0)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq = save_freq, \n",
    "                                         save_path = training_dir / 'ZIP')\n",
    "\n",
    "# train agent\n",
    "t0 = time.time()\n",
    "\n",
    "agent.learn(total_timesteps = tot_steps, \n",
    "            callback = checkpoint_callback)\n",
    "    \n",
    "t = int(time.time()-t0)\n",
    "h = int(t//3600)\n",
    "m = int((t - h*3600)//60)\n",
    "print(f\"Training elapsed time: {h:02d}h {m:02d}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 $-$ Find the best training epoch <A name=\"2.3\"> </A>\n",
    "\n",
    "The goal here is to find which file of \"saved weights\" gives the best training ?<br>\n",
    "\n",
    "We will browse the zip files of the saved weights that are in the `training_dir`: we reload the agent with the saved weights and we let the the agent control the robot to reach five successive target positions defining a diamond.\n",
    "\n",
    "For each of the 5 target positions we compute the distance between the robot end effector and the target: the best file is the one where the mean error is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processing ZIP files in <{training_dir}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 1/240          # the simulation time step\n",
    "EPSILON = 1e-3           # the distance threshold betwwen the end effector and the target\n",
    "SEED    = 1234567        # the sedd for the random generators\n",
    "MAX_EPISODE_STEPS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'env' in  dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "      \n",
    "ROBOT  = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = ROBOT, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  init_target_pos = (0.5, 0, 0.5),\n",
    "                                  reward = 'reward_1',\n",
    "                                  seed = SEED,\n",
    "                                  epsilon = EPSILON,\n",
    "                                  headless = True, \n",
    "                                  max_episode_steps = None,\n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of the saved weights files :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_files = [f for f in get_files_by_date(model_dir / 'ZIP') if f.startswith('rl_model')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We browse the training files to select the one giving the smallest mean distance \n",
    "between the robot end effector and the 5 target positions figuring a diamond.    \n",
    "\"\"\"\n",
    "q1_q2    = (113, -140)\n",
    "epsilon  = 1.e-3\n",
    "err_mean, err_std  = np.inf, np.inf\n",
    "error = []\n",
    "verbose = 0\n",
    "env._max_episode_steps = None\n",
    "\n",
    "for i, file in enumerate(list_files):            \n",
    "    print(f\">>> {file:30s}\", end=\"\")\n",
    "    res, err = [], []\n",
    "    agent = PPO.load(model_dir / 'ZIP' / file)\n",
    "    obs, _ = env.reset(options={\"dt\": DT, \n",
    "                                \"target_initial_pos\": (0.5,0,0),\n",
    "                                \"robot_initial_angle_deg\": q1_q2,\n",
    "                                \"randomize\": False,\n",
    "                                \"epsilon\": epsilon})    \n",
    "\n",
    "    for target_pos in ((0.5,0.,0.02), (1,0,0.5), (0.5,0,1), (0,0,0.5), (0.5,0,0.02)):\n",
    "        if verbose: print(f\"\\t {target_pos}\", end=\"\")\n",
    "        env.set_target_position(np.array(target_pos))\n",
    "        terminated, truncated, step_count = False, False, 0\n",
    "        while step_count < MAX_EPISODE_STEPS:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            step_count += 1\n",
    "            if terminated: break\n",
    "\n",
    "        dist_effect_target = norm(np.array(env.effector_pos) - target_pos)\n",
    "        err.append(dist_effect_target)\n",
    "        if verbose: print(f\" {step_count} steps, dist: {dist_effect_target*100:.2f} cm \")    \n",
    "\n",
    "    e_mean = np.array(err).mean()\n",
    "    e_std  = np.array(err).std()\n",
    "    error.append(err)\n",
    "    print(f\"\\t e_mean, e_std: {e_mean*100:6.2f}, {e_std*100:6.2f} cm\")\n",
    "    if e_mean < err_mean:\n",
    "        best_train = file\n",
    "        err_mean = e_mean\n",
    "        err_std  = e_std\n",
    "        \n",
    "error = np.array(error)\n",
    "\n",
    "print(f\"Best train: {best_train}, error: {err_mean*100:.2f} cm\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB\">$\\blacktriangleright$  Find the rank of the zip training file corresponding to the smallest error:</span></div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB\">$\\blacktriangleright$ Print the name of the zip training file corresponding to the smallest mean error, and the value of the smallest mean error in cm:</span></div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB;\">$\\blacktriangleright$  Plot the mean error over the 5 positions for all the zip training files:</span>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 $-$ Evaluate the trained agent performance <A name=\"2.4\"></A>\n",
    "\n",
    "Now we will display the behaviour of the robot controlled by the best trained PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = list_files[min_err_rank]\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reload the PPO network with the best ZIP file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO.load(training_dir / 'ZIP' / file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciate a fresh new robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROBOT  = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in  dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "    \n",
    "env = RoboticArm_2DOF_PyBullet(robot_urdf  = ROBOT, \n",
    "                               target_urdf = TARGET, \n",
    "                               dt = DT,\n",
    "                               init_robot_angles = (113, -140),\n",
    "                               init_target_pos = (0.5, 0, 0.5),\n",
    "                               reward = 'reward_1',\n",
    "                               seed = SEED,\n",
    "                               epsilon = EPSILON,\n",
    "                               headless = False, \n",
    "                               max_episode_steps = None,\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the robot follow the green target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "results, err_mean = {}, np.inf\n",
    "\n",
    "epsilon         = 1e-3\n",
    "nb_pts_per_line = 100\n",
    "max_step_nb     = 100\n",
    "\n",
    "# the four points defining the diamond:\n",
    "p1, p2, p3, p4 = (0.5, 0.02), (1, 0.5), (0.5, 1), (0, 0.5)\n",
    "\n",
    "# the sequence of segments defing a closed trajectory:\n",
    "diamond = ((p2, p3), (p3, p4), (p4, p1), (p1, p2))\n",
    "\n",
    "# sample the trajectory to get a list of equaly separated target points along the trajectory:\n",
    "pts, dl = sample_traj4pts(diamond, nb_pts_per_line)\n",
    "\n",
    "# Now reload the agent wit the best weights file:\n",
    "agent = PPO.load(training_dir / 'ZIP' / file)\n",
    "\n",
    "# let the agent move the robot to follow successively all the points of the trajectory:\n",
    "err = test_training(agent, env, DT, pts, max_step_nb, epsilon, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save important files and the notebook in the current training directory !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before executing the next cell, don't forget to savec your notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy precious files in experiment_dir\n",
    "for f in ('./2-DRL_training.ipynb', 'rewards.py', 'utils/tools.py', './utils/RoboticArm_2DOF.py', \n",
    "             './utils/perf.py', './urdf/RoboticArm_2DOF_2.urdf'):\n",
    "    base = os.path.basename(f)\n",
    "    shutil.copyfile(f, training_dir / base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"color: #0000BB;font-weight: bold; font-size:18pt;\">Your conclusion:</span>\n",
    "<span style=\"color: #0000BB\">\n",
    "<br><br>\n",
    "Write down your conclusion and comments on the results you have obtained in this partical work...\n",
    "</span></div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your conclusion and comments here...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
